{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c618ef4-4b93-48d8-a4bc-751af22982b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Ollama - Multi Modal Models\n",
    "In addition to the `ollama_text_generation.ipynb` notebook, this one also supports multi-modal querying - well, in this case you may add an image to your prompt, whereas multi-modal can mean also other media can be used as input.\n",
    "\n",
    "### Setup\n",
    "See the repo's tutorial to learn how to spin up the Ollama service\n",
    "\n",
    "### Available Models\n",
    "To see, what models are installed on the Raspi, run the cell in the Configuration section of this Notebook\n",
    "\n",
    "Browse models available for ollama:\n",
    "https://ollama.com/search\n",
    "\n",
    "Tested models are:\n",
    "- `moondream` (1.8b)\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6a4a4-0a26-4092-a420-78bac4bc2b34",
   "metadata": {},
   "source": [
    "#### Discuss Images\n",
    "Multi-modal models (so far, only `moondream` is tested) can accept more than just text input. In this case we pass a text prompt along with an image to the model. The prompt asks a question about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb216c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell first\n",
    "import requests\n",
    "import lib\n",
    "\n",
    "url = \"http://127.0.0.1:11434/api\"\n",
    "model = \"moondream\"\n",
    "\n",
    "stream = True # set False if you want to wait for the final result to be finished\n",
    "keep_alive = True # set True to improve speed on consecutive prompts to the same model\n",
    "# but you have to wait 5 minutes until Ollama clears its memory automatically, or manually run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Info about Model on Ollama server\n",
    "lib.check_model(url, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List locally installed models\n",
    "lib.list_models(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96f27e",
   "metadata": {},
   "source": [
    "#### without chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abdf3c3-f051-40b3-bfa7-9a560786ca09",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model\": \"moondream\",\n",
    "    \"prompt\": \"what life forms can be seen?\",\n",
    "    \"images\": [lib.image_file_to_base64(\"output/dog.png\")]\n",
    "}\n",
    "\n",
    "# https://github.com/ollama/ollama/issues/7733\n",
    "result = requests.post(f\"{url}/generate\", json=data, stream=stream)\n",
    "\n",
    "response_to_image = lib.extract_response_stream(result) if stream else lib.extract_responses(result)\n",
    "\n",
    "if not keep_alive:\n",
    "    lib.unload_model(f\"{url}/generate\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926d0a3",
   "metadata": {},
   "source": [
    "#### with chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once at the beginning of a chat\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a91a0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this for every new chat message you want to send\n",
    "\n",
    "# put every new prompt in here\n",
    "prompt = \"what is this animal and what does it do?\"\n",
    "image_file = \"output/squirrel.png\"\n",
    "last_image = None\n",
    "\n",
    "# record prompt as message\n",
    "# only add image if a new image was provided\n",
    "if image_file != last_image:    \n",
    "    messages.append(lib.to_message(\"user\", prompt, image=lib.image_file_to_base64(image_file)))\n",
    "    last_image = image_file\n",
    "else:\n",
    "    messages.append(lib.to_message(\"user\", prompt))\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"messages\": messages,\n",
    "}\n",
    "\n",
    "# https://github.com/ollama/ollama/issues/7733\n",
    "result = requests.post(f\"{url}/chat\", json=data, stream=stream)\n",
    "\n",
    "response_to_image = lib.extract_response_stream(result, history=True) if stream else lib.extract_responses(result, history=True)\n",
    "\n",
    "messages.append(lib.to_message(\n",
    "    \"assistant\", \n",
    "    lib.without_chain_of_thought(response_to_image, data[\"model\"])\n",
    "))\n",
    "\n",
    "if not keep_alive:\n",
    "    lib.unload_model(f\"{url}/generate\", data[\"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
