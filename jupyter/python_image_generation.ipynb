{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e8fa3b-a294-452b-a74f-b48765b6c491",
   "metadata": {},
   "source": [
    "### Stable Diffusion v1.5\n",
    "\n",
    "This is a legacy model from Stable Diffusion that can run on a Raspberry Pi 5 without further hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f23e6-4d71-4a14-8dec-256143570b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell before trying prompts with Stable Diffusion v1.5\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "import lib\n",
    "\n",
    "# define and initialize model, configure to run on CPU\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "pipe = pipe.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fe725-b20c-439a-b61a-cb212be4fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe your desired image here, and set number of denoising steps (higher improves quality, but takes linearly longer)\n",
    "# around 30s / Iteration on a Raspberry Pi 5\n",
    "prompt = \"a cat\"\n",
    "filename = \"image1.png\"\n",
    "inference_steps = 10\n",
    "\n",
    "stream = lib.DiffuserStream(inference_steps)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps = inference_steps,\n",
    "    callback_on_step_end = stream.write\n",
    "    ).images[0]  \n",
    "\n",
    "lib.ensure_dest_folder(\"output\")\n",
    "image.save(f\"output/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820d42a-7579-40e3-9b46-00c70effdceb",
   "metadata": {},
   "source": [
    "### SDXL-Turbo (Stable Diffusion XL)\n",
    "This is a way larger model than SD v1.5 from above. In order to make it work on a Raspberry Pi 5, check out the repo's tutorial on `dphys-swapfile` within the setup-guide for the Raspberry Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell before trying prompts with Stable Diffusion XL Turbo\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "import lib\n",
    "\n",
    "model_id = \"stabilityai/sdxl-turbo\"\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "pipe = pipe.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbc13d-d60f-4524-9290-c8bbf660ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe your desired image here, and set number of denoising steps (higher improves quality, but takes linearly longer)\n",
    "# around 2m 30s / step on a Raspberry Pi 5\n",
    "prompt = \"a dog\"\n",
    "filename = \"image1.png\"\n",
    "inference_steps = 20\n",
    "\n",
    "stream = lib.DiffuserStream(inference_steps, init_step=1)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps = inference_steps,\n",
    "    callback_on_step_end = stream.write\n",
    ").images[0]  \n",
    "    \n",
    "lib.ensure_dest_folder(\"output\")\n",
    "image.save(f\"output/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384182c",
   "metadata": {},
   "source": [
    "### DeepSeek Janus (does not work on a Raspberry Pi)\n",
    "there was no success yet, to configure it for CPU-only and also it needs further dependencies. Visit the official [Doc](https://github.com/deepseek-ai/Janus?tab=readme-ov-file#janus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2741a17-bd22-4c23-a44d-9a3b9ca852eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-1.3B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": \"A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair\",\n",
    "    },\n",
    "    {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"\",\n",
    ")\n",
    "prompt = sft_format + vl_chat_processor.image_start_tag\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1,\n",
    "    parallel_size: int = 16,\n",
    "    cfg_weight: float = 5,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()\n",
    "    for i in range(parallel_size*2):\n",
    "        tokens[i, :] = input_ids\n",
    "        if i % 2 != 0:\n",
    "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "        \n",
    "        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "\n",
    "    os.makedirs('generated_samples', exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join('generated_samples', \"img_{}.jpg\".format(i))\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)\n",
    "\n",
    "\n",
    "generate(\n",
    "    vl_gpt,\n",
    "    vl_chat_processor,\n",
    "    prompt,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
