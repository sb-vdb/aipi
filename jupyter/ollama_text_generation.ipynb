{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c618ef4-4b93-48d8-a4bc-751af22982b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Ollama - Chat\n",
    "This notebook demonstrates a range of language models available via Ollama (https://github.com/ollama/ollama).\n",
    "Ollama spins up a service that downloads and runs models for you via CLI commands or a dedicated REST API (meaning it can be easily integrated into any web app). Hence, this notebook does not use any dependencies except standard python stuff (HTTP Requests).\n",
    "\n",
    "Which language models can be used depends on their weights' size, since they have to be loaded exhaustively into the system memory. In our case, a Raspberry Pi 5 with 8GB of RAM provides roughly 7GB of memory available to an AI Model. This makes many models with 7bn parameters work - yet not all of them, since their weights' sizes (I came across) vary between 4-15GB.\n",
    "\n",
    "### Setup\n",
    "See the repo's tutorial to learn how to spin up the Ollama service\n",
    "\n",
    "### Available Models\n",
    "To see, what models are installed on the Raspi, run the cell in the Configuration section of this Notebook\n",
    "\n",
    "Browse models available for ollama:\n",
    "https://ollama.com/search\n",
    "\n",
    "Tested models are:\n",
    "- `phi3` (3.8b)\n",
    "- `deepseek-r1` (7b)\n",
    "- `llama3.2`(3.8b)\n",
    "- `codellama` (7b)\n",
    "- `moondream`(1.8b, multi-modal)\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d272a27-afea-466a-9c98-390932cc51bb",
   "metadata": {},
   "source": [
    "### Chat\n",
    "\n",
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12eb2b5-2250-4aa4-9739-dcbd937fc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell first\n",
    "import requests\n",
    "import lib\n",
    "\n",
    "url = \"http://127.0.0.1:11434/api\"\n",
    "\n",
    "model = \"deepseek-r1\" # define model here\n",
    "\n",
    "stream = True # set False if you want to wait for the final result to be finished\n",
    "keep_alive = True # set True to improve speed on consecutive prompts to the same model\n",
    "# but you have to wait 5 minutes until Ollama clears its memory automatically, or manually run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Info about Model on Ollama server / whether it is installed\n",
    "lib.check_model(url, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List locally installed models\n",
    "lib.list_models(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce588844",
   "metadata": {},
   "source": [
    "##### Demo 1: Chat without chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5499479-ebcd-46aa-ad5b-c69d99ade091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"who won the 2022 Champions League in soccer?\"\n",
    "}\n",
    "result = requests.post(f\"{url}/generate\", json=data, stream=stream)\n",
    "\n",
    "response_to_prompt = lib.extract_response_stream(result) if stream else lib.extract_responses(result)\n",
    "\n",
    "if not keep_alive:\n",
    "    lib.unload_model(f\"{url}/generate\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f593c6",
   "metadata": {},
   "source": [
    "##### Demo 2: Chat with chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once at the beginning of a chat\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab38686",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this for every new chat message you want to send\n",
    "\n",
    "# put every new prompt in here\n",
    "prompt = \"who was playing on the field?\"\n",
    "\n",
    "# record prompt\n",
    "messages.append(lib.to_message(\"user\", prompt))\n",
    "\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"messages\": messages,\n",
    "}\n",
    "\n",
    "# dispatch generation request\n",
    "result = requests.post(f\"{url}/chat\", json=data, stream=stream)\n",
    "\n",
    "response_to_prompt = lib.extract_response_stream(result, history=True) if stream else lib.extract_responses(result, history=True)\n",
    "\n",
    "#record the ai's answer\n",
    "messages.append(lib.to_message(\n",
    "    \"assistant\", \n",
    "    lib.without_chain_of_thought(response_to_prompt, model)\n",
    "))\n",
    "\n",
    "if not keep_alive:\n",
    "    lib.unload_model(f\"{url}/generate\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
